\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}

\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{booktabs} % For formal tables

\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\pdfinfo{
   /Author (Hidden for blind review)
   /Title  (underwords: Cascading Situation Assessment for Robots)
   /CreationDate (D:20180115162200)
   /Subject (Robots)
   /Keywords (Situation Assessment;Perspective Taking)
}


\usepackage{xspace}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\etal}{et al.\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\etc}{etc.\xspace}
\newcommand{\vs}{vs.\xspace}
\newcommand{\resp}{\textit{resp.}\xspace}
\newcommand{\cf}{cf.\xspace}

\newcommand{\uwds}{{\sc underworlds}\xspace}


\usepackage{amssymb}
\usepackage{amsmath}
\DeclareMathOperator{\transform}{\odot}
\DeclareMathOperator{\filter}{\ominus}
\DeclareMathOperator{\W}{\mathcal{W}}


\graphicspath{{figs/}}

\usepackage[cache]{minted}
\renewcommand{\theFancyVerbLine}{
  \sffamily\textcolor[rgb]{0.5,0.5,0.5}{\scriptsize\arabic{FancyVerbLine}}}

\newminted{python}{frame=lines,
                    linenos=true,
                    fontsize=\scriptsize,
                    xleftmargin=1.8em}

\newmintinline[python]{python}{fontsize=\footnotesize}


\begin{document}

\title{{\sc underworlds}: Cascading Situation Assessment for Robots
}




\author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

%\author{\authorblockN{Séverin Lemaignan}
%\authorblockA{Centre for Robotics and Neural Systems\\
%University of Plymouth\\
%Plymouth, United Kingdom\\
%Email: severin.lemaignan@plymouth.ac.uk}
%\and
%\authorblockN{Yoan Sallami, Aurélie Clodic and Rachid Alami}
%\authorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\authorblockN{James Kirk\\ and Montgomery Scott}
%\authorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

\begin{abstract}

    We introduce \uwds, a novel lightweight framework for \emph{cascading
    spatio-temporal situation assessment} in robotics. \uwds allows programmers
    to represent the robot's environment as real-time distributed data
    structures, containing both scene graphs (for representation of 3D
    geometries) and timelines (for representation of temporal events). \uwds
    supports \emph{cascading} representations: the environment is viewed as a
    set of \emph{worlds} that can each have different spatial and temporal
    granularities, and may inherit from each other.  \uwds also provides a set of
    high-level client libraries and tools to introspect and manipulate the
    environment models.

    This article presents the design and architecture of this open-source tool,
    and explores some applications, along with examples of use.

\end{abstract}


\IEEEpeerreviewmaketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}


\uwds is a distributed and lightweight open-source
%framework \footnote{\url{https://github.com/severin-lemaignan/underworlds}} that
framework \footnote{[GitHub URL hidden for blind review]} that
enables robot programmers to build and refine spatial and temporal
models of the environment surrounding a robot in real-time. \uwds makes it possible to share
these world models amongst the software components running on the robot.
Additionally, \uwds enables users to represent and manipulate
\emph{multiple alternatives} to the current, perceived world model in a distributed manner. For
instance, the world with some objects filtered out; the world `viewed' from the
perspective of another agent; a hypothetical world resulting from the simulated
application of a plan, etc.


\subsection{Distributed Situation Assessment}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{overview}
    \caption{Schema of a possible \uwds network: eight \emph{clients} (user-written \&
    architecture specific; in blue) are sharing environment
    models through four independent \emph{worlds} (made from joint spatial and
    temporal models). This architecture enables successive and modular
    refinement of the models (\emph{cascading} situation assessment),
    effectively adapted to each client's needs. \textit{Dashed yellow nodes represent
    other possible components in the system that do not interact directly with
    the \uwds network}.}

    \label{fig|scene}

\end{figure}


Anchoring perceptions in a symbolic model suitable for decision-making requires
perception abilities and their symbolic interpretation. We call \emph{physical
situation assessment} the cognitive skill that a robot exhibits when it
represents and assesses the nature and content of its surroundings and monitors
its evolution.

Numerous approaches exist, like amodal (in the sense of modality-independent)
\emph{proxies}~\cite{Jacobsson2008}, grounded amodal
representations~\cite{Mavridis2006}, semantic
maps~\cite{Nuechter2008, Galindo2008,Blodow2011} or affordance-based planning
and object classification~\cite{Lorken2008, Varadarajan2011}.


\uwds is specifically inspired by the geometric and temporal reasoner
{\sc Spark} (\emph{SPAtial Reasoning \& Knowledge})~\cite{sisbot2011situation}.
{\sc Spark} acts as a situation assessment reasoner that generates symbolic
knowledge from the geometry of the environment with respect to relations between
objects, robots and humans. It also takes into account the different perspective
that each agent has on the environment. {\sc Spark} embeds a
modality-independent geometric model of the environment that serves both as
basis for the fusion of the perception modalities and as bridge with the
symbolic layer~\cite{lemaignan2016artificial}. This geometric model is built
from 3D CAD models of the objects, furniture and robots, and full body, rigged
models of humans.  It is updated at run-time by the robot's sensors.
Likewise, \uwds embeds a grounded amodal model of the environment, updated
online from the robot's sensors (sensor fusion).

However, {\sc Spark} is a monolithic module that does not support sharing its
internal 3D model with other external components. In contrast, \uwds focuses on
offering a shared and distributed representation of the environment within the
robot's software architecture.

Work on distributed scene graphs~\cite{naef2003blue} has been previously applied
to robotics to provide a shared 3D representation of the robot's environment
(for instance, the \emph{Robot Scene Graph}~\cite{blumenthal2013scene} or the
\emph{Deep State Representation} proposed in~\cite{bustos2016unified}).  \uwds
offers a similar distribution mechanism for 3D scene graphs and extends it to
temporal representations. Besides, \uwds further extends this line of work by
providing the ability to create, manipulate and share \emph{multiple alternative
worlds}. As an example, these could correspond to filtered or hypothetical \emph{views} on
the initial, perceived model of the environment.


\subsection{Representing Alternative States of the Environment}

The components which make use of spatial and temporal models of the environment
are usually found in the intermediate layers of robotic architectures, between
the low-level perceptual layers, and the high-level decisional layers. They
include modules like geometric reasoners (that compute spatial and topological
relations between objects), motion planners or action recognition modules.

These components exhibit different needs in terms of representation,
like different nominal spatial and/or temporal resolutions. For instance, a 3D
motion planner would typically use coarse 3D models of surrounding objects to
lower the computational load while planning, while a module assessing the
visibility of objects might need high-resolution models 
for accurate 3D rendering. This requirement of multiple
task-specific representations has been framed as the need for \emph{deep
representations} by Beetz~\cite{beetz2010towards}.

%Spatial models are also usually large as they might
%contain a lot of geometric data (including point clouds and meshes). Efficient
%real-time sharing of such datasets is non-trivial, particularly when the said
%data is changing (\eg in the case of map building or object recognition).

Traditional robotic middlewares, like ROS, are not particularly well-suited to
deal with these different needs: full geometric data can be represented, but is
not first-class citizen (a basic task like displaying a 3D mesh at an arbitrary
position is not particularly easy to perform with ROS, requiring the combination
of static Collada meshes, a URDF kinematic description, TF broadcasters, and a
3D visualisation tool like RViz) and critically,
representing and reasoning on alternative states of the environment is not
directly feasible.

Representing alternative states is however often highly desirable. For instance,
software components manipulating environment models typically perform better if
the models are physically consistent. However, low-level perception inaccuracies
often introduce hard-to-avoid physical inconsistencies (like detected objects
floating in the air, or wrongly inset into other objects).
Therefore, a post-process stage (for instance, using a physics simulation
engine) is needed to displace the objects seen by the robot into
physically-correct positions. Implemented with a classical approach (for
instance, using ROS TF frames), we would represent an object {\tt book} with two
frames: the original frame (\eg {\tt book\_frame\_raw}) and a second one
computed by the physics engine (\eg {\tt book\_frame\_corrected}). Such an
approach leads to the robot's 3D model being cluttered with multiple frames and
does not scale well.

Another example pertains to geometric task planning: a geometric task planner
typically needs to reason over hypothetical future states of the environment
(``What happens if I move this glass onto that pile of books?'').  The planner
generates many possible future states, which in turn might require further
processing (for instance, running a physics simulation). Such a tool would
benefit a flexible representation system, where models are derived from each
other, with partial modifications and different timescales.

A third example relates to human-robot interaction scenarios where \emph{perspective
taking} is important (a prototypical example being the game `I spy with my
little eye', as implemented in~\cite{ros2010which}). Perspective taking is a
cognitive skill that relies on the ability for an agent to project itself in
others' `eyes' to estimate what they see from their perspectives. 
Perspective taking has previously been implemented in robotics by temporarily placing
virtual cameras at eye locations for each of the humans tracked by the
robot~\cite{ros2010solving}. While acceptable for simple cases, such an approach
does not maintain truly independent models of the environment for each
agent, and in particular does not permit false-belief
modelling~\cite{lemaignan2015mutual} whereas separate, independent world models
would effectively support such a skill.

Lastly, geometric \emph{pre-supposition accommodation} makes another interesting
case for alternative worlds representation. \emph{Pre-supposition accommodation}
originally comes from linguistics, where it describes the mechanism by which
\emph{context is adjusted [...] to accept [...] a sentence that imposes certain
requirements on the context in which it is
processed}~\cite{vonfintel2008presupposition}. In the context of spatio-temporal
representations, we call pre-supposition accommodation the ability of an agent
to adjust its model so that it matches some contextual constraint. For instance,
if A tells B to ``catch the red balloon behind you'', B might create a
representation of an imaginary red balloon, placed behind him, even without
actually observing the balloon: B \emph{accommodates} the \emph{pre-supposition}
of a red balloon being present behind himself. Endowing robots with this
capability has been touched upon by Mavridis \etal within their multi-modal
\emph{Grounded Situation Model}~\cite{Mavridis2006}. However, to the best of our
knowledge, a general framework which would enable robots to accommodate spatial
and temporal pre-suppositions by deriving imaginary worlds from existing ones
has not been proposed so far.

\uwds addresses this need and the main contribution of this work is a generic
approach to \textbf{represent and share multiple parallel representations of the
world}. \uwds does so by permitting clients to clone existing worlds, modify
them, and re-share them, without the cost of duplicating geometric data (as
explained in section~\ref{design}). By organising clients in a network
(Figure~\ref{fig|scene}), worlds can be made dependent on each other, resulting
in a loosely-coupled modular approach to spatio-temporal world representation
that we call \emph{cascading situation assessment}.


\section{Design and Architecture}
\label{design}

\subsection{Formal description}

An \uwds \emph{world} $\W^k$ consists in a pair $\{\mathcal{S}^k,
\mathcal{T}^k\}$.

$\mathcal{S}^k=\{N^k, T^k\}$ is a tree representing a spatial
scene (\emph{scene graph}), with $N^k$ the set of nodes and $T^k$ the set of
transformations between the nodes. $|T^k| = |N^k| - 1$ and $n^k_0 \in N^k$ is
called the root node of the scene $\mathcal{S}^k$.

$\mathcal{T}^k=\{Evt^k, Sit^k\}$ is the world's \emph{timeline}, consisting in a
set $Evt^k$ of events (duration-less states) and a set $Sit^k$ of sitations
(states with a start time and an option end time).

\uwds \emph{transformations} $\transform$ are operators that transform one world into
another. They are often unary operator (for instance, $\mathcal{W}^{k'} =
\transform\mathcal{W}^k$), but do not have to. \emph{Filters} (noted $\filter$)
are a common type of unary, antiextensive transformations such as
$\filter\mathcal{W}^k \subseteq \mathcal{W}^k$.

An \uwds \emph{network} $\mathcal{N} = \{W, O\}$ is a directed graph (cycles are
permitted) of worlds $W=\{\mathcal{W}^k\}, k\in0\ldots n$ and operators
$O=\{\transform^l\}, l \in 0\ldots m$.

\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 5cm/#1,
    level distance = 1.5cm}] 
    \node {$\mathcal{W}^0$ {\tt\footnotesize base}}
    child{ node  {$\W^1$ {\tt\footnotesize corrected}} edge from parent node[above left] {$\filter$}
            child{ node {$\W^2$ {\tt\footnotesize semantic\_map}} 
                child{ node {}} %for a named pointer
                            child{ node {}}
            }
            child{ node  {$\W^3$ {\tt\footnotesize post\_planning}}
                            child{ node {}}
                            child{ node {}}
            }                            
    }
    ; 
\end{tikzpicture}



\subsection{Cascading architecture}

Figure~\ref{fig|scene} pictures a typical \uwds topology: a graph (that
happens to be a directed acyclic graph on Figure~\ref{fig|scene}, but does not
have to be in the general case -- cycles are permitted) of \emph{clients}
connected through shared data structures called \emph{worlds}.

\subsubsection{Clients}

Software components accessing \uwds worlds are called clients. Some standard
clients (like a 3D visualisation tool) are provided with the \uwds package (see
section~\ref{std_clients}). Clients are otherwise written by the end users
using the \uwds client API (see section~\ref{api}).

Clients can both read and write onto the worlds they are connected to, and
automatically see updates broadcast by other clients connected to the same
world.

Four specific types of clients can be distinguished: \textbf{root clients} which
create and update worlds (`write-only' client, like the \emph{Sensor fusion} and
\emph{ROS interface} clients in Figure~\ref{fig|scene}); \textbf{leaf clients}
which on the contrary only read worlds, without modifying them (like the
\emph{Computation of spatial relations} client on Figure~\ref{fig|scene});
\textbf{filters} that copy an input world into an output world, performing
some filtering operation in-between (like the client \textit{Physics-based
position correction}); and \textbf{transformers} which transform one
representation into another (like the \emph{Semantic mapping} client on
Figure~\ref{fig|scene}).

\subsubsection{Worlds}

Worlds are effectively distributed data structures composed of a scene graph
representing the 3D geometry of the environment, and a timeline storing temporal
events.

Each world is technically independent from all others. Dependencies between
worlds arise from the clients' connections. For instance, filters effectively
create a dependency between worlds. On Figure~\ref{fig|scene}, the \textit{Physics-based
position correction} client creates a dependency between the world {\tt base} (which
represents here the result of raw sensor fusion) and the world {\tt corrected}
which would be a physically-consistent copy of {\tt base}.
As a result, an \uwds network can be seen as a dependency graph between worlds (where
cyclic dependencies are permissible).

This architecture enables what we call \emph{cascading situation assessment}:
independent software components (the clients) build, refine and share successive
models of the environment by a combination of filtering/transformations steps
and model branching.

\subsubsection{Scenes}

Worlds contain both a geometric model and a temporal model. The geometric
model is represented as a scene graph. The scene graph has a unique root node,
to which a tree of other nodes is parented.

Nodes in an \uwds scene graph have four possible types: \textbf{objects} that
represent concrete physical objects (typically with one or several associated 3D
meshes); \textbf{entities} that represent abstract entities like reference
frames or groups of objects; \textbf{perspectives} that represent viewpoints
of the scene (like cameras or human gaze); and \textbf{fields} that represent
scalar or vector fields (like the visibility of an object, the working space of
robot, \etc -- note that fields are not yet implemented in the current version
of \uwds, see section~\ref{futurework}).

Every node has a unique ID, a parent, a 3D transformation relative to the parent
and an optional name. \emph{Object} nodes optionally store as well pointers to
their associated meshes. Importantly, mesh data (or other geometric datasets
like point clouds) are \emph{not} stored within the nodes themselves. \uwds
represents geometric data as immutable data, identified by their hash value
(preventing \textit{de facto} data duplication).  Nodes only store the hash
corresponding to the desired geometric data, and the actual data is pulled from
the server by the clients whenever they actually need it (for rendering for
instance).

\subsubsection{Timelines}

Complementing the spatial representation encapsulated in the scene graph, each
world also stores the world's \emph{timeline}. This data structure is shared
and synchronised amongst the clients in the same way as the scene graph.
Clients can record both \emph{events} (duration-less states) and
\emph{situations} in the timeline, \ie states with a start time and a (possibly open-ended) end time.

Importantly, the \uwds server automatically generates a snapshot of
the scene graph whenever an event or situation is added to the timeline. The
snapshot is associated to the event, which allows clients to effectively retrieve past
states of the world. This capability is anticipated to be primarily used by \uwds clients
performing action recognition.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{uwds-screenshot}
    \caption{Screenshot of the {\tt uwds view} 3D visualisation and manipulation
    client. In this particular example, the 3D meshes have been pre-loaded using
    {\tt uwds load}. Their positions are then updated at run-time using the
    robot's sensors and proprioception (joint state).}
    \label{fig|uwds-view}
\end{figure}

\subsection{Distributed spatio-temporal models}
\label{arch}

\uwds is not a monolithic piece of software. Instead, it stands for both a
\emph{network of interconnected clients} which manipulate spatial and temporal
models of the robot environment (for instance, a motion planner, a object
detection module, a human skeleton tracker, etc.), and for a {client library}
that makes it possible to interface existing software components with the network.

Critically, the network is essentially hidden to the client: from the user
perspective, the environment model is manipulated as a local data structure (see
Listing~\ref{lst|pythonapi}). Modifications to the model are asynchronously synchronised with
a central server (the {\tt underworlded} daemon) and broadcast to every other
client connected to the same world.
% JK: underworlded - intentional? (I guess yes, but checking)

As previously mentioned, worlds are composite data structures comprised of a
scene graph and a timeline. These data structures are synchronised using
Google's gRPC message passing framework\footnote{\url{http://www.grpc.io/}}, ensuring
high throughput, reliability and cross-platform/cross-language support. The \uwds
API is specifically discussed hereafter, in section~\ref{api}.


\uwds is meant to broadcast complex environment representations (typically
including large geometric datasets, like meshes) in real-time. \uwds itself does
not perform many CPU intensive tasks (CPU intensive processing tasks -- sensor
fusion, physics simulation, \etc -- are performed by the clients themselves) and
as such, the performance bottleneck is essentially the network's data
throughput.  In that regard, one of the simple yet critical optimisations
performed by \uwds is automatic caching of mesh data. Mesh data are not
transmitted when nodes are updated; only a hash value of the mesh data. The
client can then request the full data whenever it is actually needed.



\section{API \& Clients}

\subsection{API}
\label{api}

As mentioned, \uwds uses Google's gRPC as message passing protocol. The protocol
is explicitly defined (using the \emph{protocol
buffers}\footnote{\url{https://developers.google.com/protocol-buffers/}}
interface definition language), and bindings to various languages and platforms
can be automatically generated from the protocol definition file (as of Sept
2017, gRPC can generate bindings for C, C++, C\#, Node.js, PHP, Ruby, Python, Go
and Java, on Windows, Mac, Linux and Android).  The cross-platform/cross-language support
of gRPC is especially welcome in the academic context, as it offers ease and
flexibility to plug a variety of pre-existing components into an \uwds network.


However, the gRPC message passing layer is low-level with respect to the typical
use of \uwds (manipulation of asynchronous, distributed spatio-temporal models
of the robot environment). In particular, the asynchronous fetching (and
conversely, remote updating) of nodes and time-related objects is typically
hidden from the user, and managed instead by the \uwds client library.

\uwds currently offers such a high-level client library for Python only (a C++
library is under development).  Listing~\ref{lst|pythonapi} gives a complete
example of an \uwds client performing simple filtering: the client continuously
listens for changes in an input world, removes some objects (in this case, items
whose volume is below a threshold), and forwards all other changes to
an output world, effectively making the output world a copy of the input world
with all smaller objects removed.

\begin{listing}[h!]

\begin{pythoncode}
import underworlds

# by default, connect to the server on localhost
with underworlds.Context("small_object_filter") as ctx:

    in_world = ctx.worlds["example_in"]
    out_world = ctx.worlds["example_out"]

    while True:

        in_world.scene.waitforchanges()

        for node in in_world.scene.nodes:
            if node.volume > THRESHOLD:
                out_world.scene.nodes.update(node)


\end{pythoncode}
    \caption{Example of a simple yet complete \uwds filter, written in Python:
    the client connects to the \uwds network, blocks until the world {\tt
    example\_in} changes, and only propagate nodes that match the condition to
    the world {\tt example\_out}.}

    \label{lst|pythonapi}
\end{listing}

\subsection{Standard Clients}
\label{std_clients}

The \uwds package provides several standard clients to perform common tasks on
\uwds networks.

\subsubsection{3D Visualisation and manipulation}

Interestingly, while \uwds deals with 3D geometries and scenes, it does
represent 3D entities purely as data structures; no visual representation is
involved (and as such, the \uwds server and core libraries do not depend on any
graphics library like OpenGL). However, for all practical purposes, the ability
to visualise the content of a scene is desirable. \uwds provides a standard
client, {\tt uwds view}, that performs real-time 3D rendering of worlds,
using OpenGL (Figure~\ref{fig|uwds-view}).

This tool also supports basic object manipulations (translations, rotations),
that are broadcast to the other \uwds clients connected to the same world.

\paragraph*{Assets loading}

Geometric data like meshes can be loaded into a world (or rather, into the whole
network as geometric data are treated as immutable resources shared amongst all
worlds) by simply sending an array of vertices and faces to the \uwds server.
As such, any client can load 3D data that is automatically made available to the
whole network. For example, an object recognition module recognises a new
object. It sends the mesh to the \uwds server and creates a new node which
references the mesh. At the next frame update, the 3D viewer sees a new node
referencing an unknown mesh. The viewer requests the mesh data from the
server, stores it internally (\eg as an OpenGL buffer), and renders it. Whenever
the node transformation is updated, the 3D viewer can re-render the node without
having to download the mesh from the server again.

Often, objects manipulated by the robot have known meshes with corresponding CAD
models that can be conveniently pre-loaded. In these cases, \uwds provides a
tool, {\tt uwds load}, that loads a mesh into a \uwds network (and optionally,
creates a node) from a large range of 3D formats (including Collada, FBX, OBJ,
Blender)\footnote{The underlying import
capability is provided by the {\sc assimp} library.
\url{http://assimp.sourceforge.net/}}.

\subsubsection{Introspection and debugging}

\uwds provides a range of tools to inspect a running network. Graphical
tools ({\tt uwds explorer} and {\tt uwds timeline}, see Figure~\ref{fig|explorer})
provide a user-friendly overview of the system's graph with the connections
between the clients and the worlds, as well as their activity.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{tools}
    \caption{Screenshots of the timeline (top) and network (bottom)
    introspection tools. Worlds are represented as boxes; clients as ellipses.}
    \label{fig|explorer}
\end{figure}

Specialised command-line tools are also available to list the worlds
and their content ({\tt uwds ls}) at run-time, or to display detailed information for a
specific node ({\tt uwds show}).

\subsubsection{Interface with ROS}

\uwds is meant to integrate as easily as possible into existing robot
architectures, and interfaces transparently with ROS' TF frame system through
the {\tt uwds tf} client.

The {\tt uwds tf} client continuously monitors the ROS TF tree, and mirrors
TF frames as nodes in the desired \uwds world. A node is first created if none
matches a given TF frame, and its transformation is subsequently updated,
mirroring the TF frame. A regular expression can be provided to only mirror a
subset of the TF tree into \uwds.

Currently, the process is unidirectional: the {\tt uwds tf} client performs TF
to \uwds updates, but not the reverse.

\subsection{Spatial Reasoning and Perspective Taking}

Spatial reasoning~\cite{O'Keefe1999} is a field in its own right, and has been
used for natural language processing for applications such as direction
recognition~\cite{Kollar2010,Matuszek2010} or language
grounding~\cite{Tellex2010}. Other examples in human-robot interaction include Ros
\etal~\cite{ros2010solving, ros2010which} which has recently been integrated
into a full architecture for autonomous human-robot
interaction~\cite{lemaignan2016artificial}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{spatialrelations}
    \caption{The {\tt spatial\_relations} client computes perspective-aware
    spatial relations between objects and agents: \emph{allo-centric} relations (like
    {\tt is in} or {\tt is on}) are independent of the viewpoint, while
    \emph{ego-centric} relations ({\tt in front of}, {\tt left of}) depend on
    the viewer perspective.}
    \label{fig|spatialrelations}
\end{figure}

\uwds provides a client ({\tt spatial\_relations}) to compute both
allo-centric and ego-centric (\ie viewer-dependent) spatial relations
between objects (Figure~\ref{fig|spatialrelations}).


\uwds also implements an efficient algorithm to assess object visibility from a
specific viewpoint (\ie from a given \emph{perspective} node). The algorithm (color
picking) enables fast (single pass) computation of the visibility of every
object in the scene, while providing control regarding how many pixels should be actually
visible for the object to be considered globally visible. The
command-line tool {\tt uwds visibility} returns the list of visible objects from
the point of view of each camera in a given world, and \uwds also provides the
helper class {\tt VisibilityMonitor} to programmatically access visibility
information.

When integrated into a filter node, visibility computation allows easy
creation of new worlds representing the estimated perspectives of the different
agents. Listing~\ref{lst|humanperspective} provides the complete code of such an
\uwds client.


\begin{listing}[h!]
\begin{pythoncode}
import copy
import underworlds
from underworlds.tools.visibility import VisibilityMonitor

with underworlds.Context("Human PoV") as ctx:

    source = ctx.worlds["base"]
    target = ctx.worlds["human_perspective"]

    # pick up the first node named 'human'
    human = source.scene.nodebyname("human")[0]
    target.scene.nodes.append(human)

    # VisibilityMonitor computes the set of visible 
    # nodes from a given viewpoint
    visibility = VisibilityMonitor(ctx, source)

    node_mapping = {}

    while True:

        for node in visibility.from_camera(human):

            newnode = node.copy()

            # track the correspondence between nodes
            # in source and target scenes
            if node in node_mapping:
                newnode.id = node_mapping[node].id
            else:
                node_mapping[node] = newnode

            # reparent the nodes to the new root
            if node.parent == source.scene.rootnode.id:
                newnode.parent = target.scene.rootnode.id

            target.scene.nodes.update(newnode)

        source.scene.waitforchanges()

\end{pythoncode}
    \caption{Example of an \uwds client that creates a world named {\tt
    human\_perspective} from a source world {\tt base} by only updating objects
    that are in the field of view of the human. As such, {\tt human\_perspective} is
    a real-time model of the human perspective on the scene.}

    \label{lst|humanperspective}
\end{listing}


\section{Performances and Application Example}
\label{application}

\subsection{Performances}
\label{performances}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{performances}
    \caption{Propagation times of one change (node creation)
    across $n$ worlds. The test is performed by running $n-1$ pass-through
    filters (independent processes) which each monitor one world and replicate
    any change into the next world. Times measured over 20 trials, performed on
    a 8 core machine.}
    \label{fig|performances}
\end{figure}

The main performance metric of \uwds is the \emph{propagation time}: the time is
take of a change in a given world to cascade to all the dependant 

\subsection{Application to service robots and human-robot interaction}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{l2tor-photo2-blind}
    \caption{TODO LAAS: update photo -> MuMMER picture?}
    \label{fig|mummer-example}
\end{figure}

%\uwds has been used within the European MuMMER project
\uwds has been used within the [hidden for blind review] project
in order to conceptualise and visualise the spatial relations and visibility of
the physical objects that participants interact with.

TODO LAAS

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{l2tor}
    %\caption{Schema of the \uwds architecture used in the MuMMER project. }
    \caption{TODO LAAS update archi -- Schema of the \uwds architecture used in the [hidden for blind review] project. }
    \label{fig|mummerarchitecture}
\end{figure}

The interaction scenario requires a number of \uwds clients. The clients are
distributed across...

Figure~\ref{fig|mummerarchitecture} illustrates the topology of
the \uwds network...

TODO LAAS

\section{Discussion and Conclusion}

\subsection{Relation to existing robotic middleware}

Like traditional robotic middleware, \uwds offers a form of distributed
computation based on message passing. However, it distinguishes itself from
existing middleware in several significant ways.

First and foremost, \uwds purposefully does not offer any \emph{general}
capability to distribute computation and data flow amongst independent
components: it focusses specifically on distributing environment models, both
spatial (geometric models) and temporal (events and situations).

While using standard middleware as \emph{underlying transport} for \uwds would be
technically feasible and relatively easy to implement, it does not offer any
clear advantage over lighter and dedicated message passing libraries like ZeroMQ
or gRPC (the later being the one used by \uwds).

\subsection{Future work}
\label{futurework}

As illustrated in section~\ref{application}, \uwds is already deployed and used
on the field. Several features are however still under development.

\subsubsection{Representation capabilities} as presented in
section~\ref{design}, the current version of \uwds allows to represent
\emph{objects}, \emph{abstract entities} like groups and \emph{perspectives}.
\emph{Fields} are also part of the \uwds design, but are not yet implemented.

\paragraph*{Representation of fields} fields are commonly used to represent
continuously-valued spatial entities. Fields might or might not be spatially
bounded. Examples include the working space of a robot arm (spatially bounded),
the field of view of a camera (spatially bounded), proxemics (potentially
unbounded). We plan to represent fields in \uwds using the memory-efficient
octomaps~\cite{hornung2013octomap}. Similarly to geometric data, octomaps
will not be directly stored with the nodes (nodes will refer to them through 
handles), but unlike geometric data, they will not be treated as immutable
datasets by the server, permitting real-time updates.

\paragraph*{Representation of uncertainty} currently node positions are 
stored as $4\times4$ transformation matrices, relative to the node parent. This
representation is efficient, and conveniently matches traditional representation
systems (including ROS TF frames or OpenGL transformations). However, the explicit management
of uncertainties is instrumental to many robotic applications, and we plan to
add full support for position uncertainties to \uwds. We plan to add this
support by adding a pose covariance matrix to the nodes, and equipping the
different \uwds helper tools with corresponding support (like covariance
ellipses visualisation in {\tt uwds view}).


\subsubsection{Manipulation of representations} \uwds makes it possible to
cheaply clone and modify multiple worlds. However, little is provided to
jointly manipulate sets of worlds. In that regard, we plan to add a mechanism to
efficiently \emph{compare} two worlds. This would permit a client
to quickly identify modifications applied to a world, and could prove especially
useful when combined with temporal events to answer questions like ``what has
changed in the scene?''. Comparisons between worlds involves the definition of
an efficient fuzzy \emph{diff} algorithm for 3D scene graphs, able to
selectively ignore non-relevant micro displacements.

\subsubsection{Implementation and Integration} finally, we plan to continue to
improve the integration of \uwds into existing software architectures. A
short-term goal is to provide excellent C++ support, with a high-level,
user-friendly C++ client library. This is critical for a broader adoption of \uwds
within the robot community. Support for other languages might follow, depending
on demands and open-source contributions.

In term of implementation, we are also investigating alternative back-ends that
could compete with gRPC to provide the synchronisation mechanisms between
clients. Due to the high-level nature of \uwds (a set of shared, tree-like data
structures), the use of the GIT version control system is one particular (if
unusual) option being studied. The scene graph would be represented as a
hierarchy of directories, nodes would be files in these directories, creating
new worlds would consist in creating a branch in the repository, and sharing
would be supported through the usual pull/push GIT mechanisms. Initial
prototyping shows that under the right conditions (in particular, an in-memory
file system), performance is not an issue. Automatic conflicts resolution, on
the other hand, might prove more challenging.


\subsection{Conclusion}

We have introduced \uwds, a novel framework for shared and composable
spatio-temporal representations of a robot's world. The key contributions of our
approach are: a composite data structure for environment representation within a
robotic software architecture, made of a scene graph and a timeline; a mechanism
to efficiently and transparently share this data structure amongst a set of
clients (the software modules of the robot); a cascading architecture permitting
the explicit of representation of alternative states of the world while
maintaining a network of dependencies; a typology of clients
(\emph{root}, \emph{leaf}, \emph{filter}, \emph{transformer}) to systematically
describe such a network of representations dependencies.

We have additionally presented a concrete instantiation of a system relying on \uwds
for its representation needs, and we have sketched future directions of
development.

We believe this work can practically support existing robotic architectures with
state-of-the-art spatio-temporal representation capabilities. We also hope that
this line of research can lead to a better understanding of the representation
needs of modern robotic systems, and participate to the emergence of a possible
common representation platform for robots, building on previous formalisation
efforts like the RSG-DSL domain specific language~\cite{blumenthal2014towards}.

\section*{Acknowledgment}

%This work has been supported by the EU H2020 Marie Sk\l odowska-Curie Actions
%project DoRoThy (grant 657227) and the EU H2020 L2TOR project (grant 688014).

%The authors also acknowledge the contribution of the HRI lab of LAAS-CNRS (where this
%work debuted), and in particular, the influential ideas of Rachid Alami.

[Hidden for blind review]

\bibliographystyle{plainnat}
\bibliography{bibliography}




\end{document}
